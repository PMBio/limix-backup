\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{color}
\usepackage[affil-it]{authblk}
\usepackage{multirow}
\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{pdfsync}

%\usepackage[nofiglist, notablist, nomarkers]{endfloat}
\input{utils.tex}
%\title{Probabilistic learning of confounding factors in genetical
%genomics studies}
\title{Derivations and mathematical details of GPmix}
\author[1]{Christoph Lippert, Oliver Stegle}

\affil[1]{Department Empirical Inference,
Max Planck Institutes T\"ubingen, Germany}
\date{}

\captionsetup[subfloat]{listofformat=parens}
\newcommand{\OLI}[1]{{\color{blue}\fbox{OLI} #1}}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version
%\newcommand{\B}[1]{\bm{#1}} Christoph:removed this to make things consistent
\newcommand{\B}[1]{{\bf{#1}}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\norma}[1]{\mathcal{N}\left(#1\right)}
\newcommand{\eref}[1]{(\ref{#1})}
\renewcommand{\R}{\mathbb{R}}
\newcommand\norm[1]{\left\Vert {#1} \right\Vert}
\newcommand\rank{\mathrm{rank}}
\newcommand\tr{\mathrm{Tr}}
\newcommand\Normal[3]{\normal{#1}{{#2}\;;\;{#3}}}
\newcommand\ve[1]{\text{vec}\left(#1\right)}
\newcommand\Real{\mathbb{R}}
\newcommand{\Ykron}{\B{U}_R^\T\B{Y}\B{U}_C}
\newcommand{\XWAkron}[1]{\B{U}_R^\T\B{X}_{#1}\B{W}_{#1}\B{A}_{#1}\B{U}_C}
\newcommand{\XWAkronT}[1]{\B{U}_C^\T\B{A}_{#1}^\T\B{W}_{#1}^{\T}\B{X}_{#1}^{\T}\B{U}_R}
\begin{document}
\maketitle


\section{Some definitions}
\begin{itemize}
\item $\B{A}=\B{U}\B{S}\B{U}^{\T}$ is the eigenvalue decomposition of
  the symmetric $D$-by-$D$ matrix $\B{A}$, where $\B{U}$ is an
  $D$-by-$D$ orthonormal matrix, holding the $D$ eigenvectors of
  $\B{A}$ and $\B{S}$ is an $D$-by-$D$ diagonal matrix, holding the
  corresponding eigenvalues of $\B{A}$ as diagonal entries. 
\item $\B{A}\odot\B{B}$ is the pointwise or Hadamard product of $\B{A}$ and $\B{B}$.
\item $\B{A}\otimes\B{B}$ is the Kronecker product of $\B{A}$ and $\B{B}$.
\item $\B{Y}\in\mathbb{R}^{N\times G}$ is the matrix holding all samples, having $N$ rows and $G$ columns.
\end{itemize}


\section{Kronecker testing strategies and models}
$\B{A}_j\in\Real^{C\times M_j}$ is a matrix, that replicates the
$j$-th fixed effects matrix $\B{X}_j\in\Real^{R\times D_j}$. $\B{A}_j^\T$
typically would be a binary matrix, but could in principle be
anything. Using different versions of $\B{A}_j^\T$ corresponds to chosing
a testing strategy. For example, when $\B{A}_j^\T$ is the $C\times C$
Identity matrix, then one would fit an independent weight to every
column of $\B{Y}$, when $\B{A}^\T_j$ is a row-vector of ones, then one
would fit a single joint weight to all columns of $\B{Y}$.
\begin{equation}
\Normal{\ve{\B{Y}}}{\sum_j\B{A}^{\T}_j\otimes\B{X}_j\ve{\B{W}_j}}{\B{C}\otimes\B{R}
+ \sigma^2\B{I}}
\end{equation}
As long as $D_j\leq R$, $M_j\leq C$, the rank of $\B{X}_j$ is $D_j$
and the rank of $\B{A}_j$ is $M_j$, and the rank of
$\left[\B{X}_1...\B{X}_J\right]$ is $\sum_{j=1}^JM_j$ the number of
degrees of freedom of a single $\B{W}_j$ is $D_j \cdot M_j$
(sufficient condition).
\section{Efficient computation of tensor GP models}
\begin{equation}
\log\Normal{\ve{\B{Y}}}{\sum_{j=1}^J\B{A}^{\T}_j\otimes\B{X}_j\ve{\B{W}_j}}{\B{C}\otimes\B{R}
+ \sigma^2\B{I}}
\end{equation}
Apply the vec-trick to the mean term:
\begin{equation}
\log\Normal{\ve{\B{Y}}}{\ve{\sum_{j=1}^J\B{X}_j\B{W}_j\B{A}_j}}{\B{C}\otimes\B{R}
+ \sigma^2\B{I}}
\end{equation}
\begin{equation}
-\frac{C\cdot R}{2}\log(2\pi)-\frac{1}{2}\log|\B{C}\otimes\B{R} +
\sigma^2\B{I}|
-\frac{1}{2}\ve{\B{Y} -\sum_{j=1}^J\B{X}_j\B{W}_j\B{A}_j}^\T\left(\B{C}\otimes\B{R} +
\sigma^2\B{I}\right)^{-1}\ve{\B{Y} -\sum_{j=1}^J\B{X}_j\B{W}_j\B{A}_j}
\end{equation}
\subsection{Derivative of the squared form wrt. $\B{W}$}
\begin{equation}
\frac{\partial}{\partial [\B{W}_k]_{ab}}\left(-\frac{1}{2}\ve{\B{Y} -\sum_{j=1}^J\B{X}_j\B{W}_j\B{A}_j}^\T\left(\B{C}\otimes\B{R} +
\sigma^2\B{I}\right)^{-1}\ve{\B{Y} -\sum_{j=1}^J\B{X}_j\B{W}_j\B{A}_j}\right)
\end{equation}
We define matrix $\B{D}\in\Real^{R\times C}$, such that $\ve{\B{D}} = \diag\left(\B{S}_C\otimes\B{S}_R +
\sigma^2\B{I}\right)^{-1}$ and rotate the data:
\begin{equation}
\frac{\partial}{\partial [\B{W}_k]_{ab}}
-\frac{1}{2}\tr\left(\left(\Ykron
    -\sum_{j=1}^J\XWAkron{j}\right)^\T\left(\left(\Ykron
      -\sum_{j=1}^J\XWAkron{j}\right)\odot \B{D}\right)\right)
\end{equation}
\begin{eqnarray}\nonumber
\frac{\partial}{\partial [\B{W}_k]_{ab}}
-\frac{1}{2}\tr\left(\Ykron^\T (\Ykron \odot \B{D})\right)
    +\tr\left( \sum_{j=1}^J\XWAkronT{j}(\Ykron \odot \B{D})\right) \\
-\tr\left( \sum_{j=1}^J\sum_{i=j+1}^J\XWAkronT{j}(\XWAkron{i} \odot
  \B{D}) \right) 
-\frac{1}{2}\tr\left(\sum_{j=1}^J\XWAkronT{j}(\XWAkron{j} \odot \B{D}) \right)
\end{eqnarray}
Leaving out all terms without $\B{W}_k$:
\begin{eqnarray}\nonumber
\frac{\partial}{\partial [\B{W}_k]_{ab}}
    \tr\left( \XWAkronT{k}(\Ykron \odot \B{D})\right) \\
-\tr\left( \sum_{j\neq k}\XWAkronT{k}(\XWAkron{j} \odot
  \B{D}) \right) 
-\frac{1}{2}\tr\left(\XWAkronT{k}(\XWAkron{k} \odot \B{D}) \right)
\end{eqnarray}

\begin{eqnarray}\nonumber
    \tr\left( {\B{U}_C^\T[\B{A}_{k}]_{b:}^{\T}[\B{X}_{k}]_{:a}^{\T}\B{U}_R} (\Ykron \odot \B{D})\right) \\
-\tr\left( \sum_{j\neq k}{\B{U}_C^\T[\B{A}_{k}]_{b:}^{\T}[\B{X}_{k}]_{:a}^{\T}\B{U}_R} (\XWAkron{j} \odot
  \B{D}) \right) 
-\tr\left({\B{U}_C^\T[\B{A}_{k}]_{b:}^{\T}[\B{X}_{k}]_{:a}^{\T}\B{U}_R} (\XWAkron{k} \odot \B{D}) \right)
\end{eqnarray}
Moving around the terms in the trace and combining the last two traces
into one sum:
\begin{eqnarray}\nonumber
    \tr\left( [\B{X}_{k}]_{:a}^{\T}\B{U}_R (\Ykron \odot \B{D}) \B{U}_C^\T[\B{A}_{k}]_{b:}^{\T}\right) \\
-\tr\left( \sum_{j=1}^J{[\B{X}_{k}]_{:a}^{\T}\B{U}_R} (\XWAkron{j} \odot
  \B{D}) \B{U}_C^\T[\B{A}_{k}]_{b:}^{\T}\right) 
\end{eqnarray}
Observing that this is a scalar, we can eliminate the trace and re-arrange:
\begin{eqnarray}
    [\B{X}_{k}]_{:a}^{\T}\B{U}_R ((\Ykron-\sum_{j=1}^J \XWAkron{j}) \odot \B{D}) \B{U}_C^\T[\B{A}_{k}]_{b:}^{\T} \\
\end{eqnarray}
Stacking together these terms for all $a$ and $b$, the gradient becomes
\begin{eqnarray}
    \B{X}_{k}^{\T}\B{U}_R ((\Ykron-\sum_{j=1}^J \XWAkron{j}) \odot \B{D}) \B{U}_C^\T\B{A}_{k}^{\T} \\
\end{eqnarray}

\newpage
\end{document}
